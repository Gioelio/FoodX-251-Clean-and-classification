{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity_search_gui import load_images_features, find_similar_images\n",
    "from misc import load, load_class_labels\n",
    "from data_loader import data_loader;\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_root = \"../dataset/\"\n",
    "test_info = load(dataset_root + 'labels/test.csv')\n",
    "test_dir = dataset_root + 'val_set/'\n",
    "\n",
    "dl_test = data_loader(test_info, test_dir, 1, (224, 224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn, _ = load_images_features(False)\n",
    "\n",
    "def evaluate(base_weight):\n",
    "    results = []\n",
    "    min_cut = 60;\n",
    "\n",
    "    for i in tqdm(range(dl_test.number_of_batch())):\n",
    "        _, labels, filenames = dl_test.get_batch(i);\n",
    "        query_label = labels[0];\n",
    "        query_filename = filenames[0];\n",
    "        query_filename = dataset_root + 'complete/' + query_filename;\n",
    "\n",
    "        most_similar_filenames = find_similar_images(query_filename, None, nn, False, True, base_weight=base_weight, verbose=0);\n",
    "        most_similar_filenames = np.array([file.split('/')[-1] for file in most_similar_filenames]);\n",
    "        mask = [('val' in file) for file in most_similar_filenames];\n",
    "        most_similar_filenames = most_similar_filenames[mask];\n",
    "\n",
    "        cut_point = min(len(test_info[test_info['label'] == query_label].values), min_cut)\n",
    "        most_similar_filenames = most_similar_filenames[:cut_point];\n",
    "        \n",
    "        correct = 0\n",
    "        wrong = 0 \n",
    "        for file in most_similar_filenames:\n",
    "            label_found = test_info[test_info['filename'] == file]['label'].values[0]\n",
    "            if label_found == query_label:\n",
    "                correct += 1;\n",
    "            else:\n",
    "                wrong += 1\n",
    "        results.append((correct/(correct + wrong), (correct + wrong)))\n",
    "        #if i % 10 == 0:\n",
    "        #    print(correct, wrong)\n",
    "\n",
    "        total_element = 0\n",
    "        acc = 0\n",
    "        for res in results:\n",
    "            perc, weight = res;\n",
    "            total_element += weight;\n",
    "            acc += perc * weight;\n",
    "        acc /= total_element\n",
    "            \n",
    "    return results, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with only efficient net base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, acc = evaluate(1)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with ensembling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, acc = evaluate(0)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
