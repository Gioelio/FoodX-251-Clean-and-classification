{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import load, load_class_labels\n",
    "from data_loader import data_loader;\n",
    "from tensorflow import keras;\n",
    "\n",
    "dataset_root = \"./dataset/\"\n",
    "\n",
    "train_dir = dataset_root + \"train_set/\"\n",
    "val_dir = dataset_root + \"val_set/\"\n",
    "val_degraded_dir = dataset_root + \"val_set_degraded/\"\n",
    "\n",
    "sample_rate = 1\n",
    "train_info = load(dataset_root + 'labels/train.csv', 1, sample_rate)\n",
    "val_info = load(dataset_root + 'labels/validation.csv')\n",
    "\n",
    "train_image_names = train_info.iloc[:, 0].values\n",
    "val_image_names = val_info.iloc[:, 0].values\n",
    "\n",
    "train_labels = train_info.iloc[:, 1].values\n",
    "val_labels = val_info.iloc[:, 1].values\n",
    "\n",
    "class_labels = load_class_labels(dataset_root + 'classes.txt')\n",
    "\n",
    "dl = data_loader(train_info, train_dir, 400, (224, 224));\n",
    "dl_val = data_loader(val_info, train_dir, 400, (224, 224));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "from handcrafted import extract_features, color_histograms, sift, gabor_response, lbp\n",
    "from tqdm import tqdm;\n",
    "import numpy as np;\n",
    "\n",
    "sampling = train_info.sample(frac=0.5)\n",
    "images_names = sampling.iloc[:, 0].values\n",
    "\n",
    "pool = ThreadPool(processes=14)\n",
    "\n",
    "angles = np.arange(0, np.pi, np.pi/4)\n",
    "lambdas = np.arange(0, 1, 0.2);\n",
    "gammas = [0.5]\n",
    "max_features = 50\n",
    "\n",
    "sift_features = pool.apply_async(extract_features, (train_dir, images_names,\n",
    "                                                  [(lambda img: sift(img, max_features=max_features))]))\n",
    "\n",
    "gabor = pool.apply_async(extract_features, (train_dir, images_names,\n",
    "                                                  [(lambda img: gabor_response(img, (10, 10), angles, [5], lambdas, gammas))]))\n",
    "\n",
    "color = pool.apply_async(extract_features, (train_dir, images_names,\n",
    "                                                  [(lambda img: color_histograms(img))]))\n",
    "\n",
    "lbp_features = pool.apply_async(extract_features, (train_dir, images_names,\n",
    "                                                  [(lambda img: lbp(img))]))\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "sift_features = sift_features.get()\n",
    "gabor = gabor.get()\n",
    "color = color.get()\n",
    "lbp_features = lbp_features.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import unroll_arrays\n",
    "import bag_of_words as bow\n",
    "\n",
    "prob = 1\n",
    "voc_size = 300\n",
    "\n",
    "unrolled = unroll_arrays(sift_features, 1);\n",
    "\n",
    "kmeans = bow.fit(unrolled, vocabulary_size=voc_size, verbose=True, n_init=1, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_features = bow.predict(kmeans, sift_features)\n",
    "print(bow_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_color = np.array(color)\n",
    "unrolled_color = unrolled_color.reshape(unrolled_color.shape[0], -1)\n",
    "unrolled_gabor = np.array(gabor)\n",
    "unrolled_lbp = np.array(lbp_features)\n",
    "\n",
    "all_features = np.concatenate((unrolled_color, bow_features, unrolled_gabor, unrolled_lbp), axis=1)\n",
    "print(all_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(all_features, sampling['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=390),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(251, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "epoch_num = 40;\n",
    "\n",
    "labels = keras.utils.to_categorical(sampling['label'].values.tolist(), num_classes=251)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    #df = df.sample(frac=1);\n",
    "    model.fit(all_features, labels, batch_size=100, shuffle=True, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for epoch in range(epoch_num):\n",
    "    dl.shuffle_dataframe();\n",
    "    for batch_num in tqdm(range(dl.number_of_batch())):\n",
    "        (images, labels, _) = dl.get_batch(batch_num)\n",
    "        features = []\n",
    "        for image in images:\n",
    "            image = np.float32(image)\n",
    "            hist = color_histograms(image);\n",
    "            features.append(hist)\n",
    "        \n",
    "        features = bow.predict(kmeans, features);\n",
    "        \n",
    "        labels = keras.utils.to_categorical(labels, num_classes=251)\n",
    "        model.train_on_batch(features, labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val.shuffle_dataframe()\n",
    "for batch_num in tqdm(range(dl_val.number_of_batch())):\n",
    "    (images, labels, _) = dl_val.get_batch(batch_num)\n",
    "    features = []\n",
    "    for image in images:\n",
    "        image = cv.normalize(image, None, 0, 255, cv.NORM_MINMAX).astype('uint8')\n",
    "        image = image[:, :, ::-1];\n",
    "        #sift_info = sift(image, max_features=max_features)\n",
    "        features.append(sift_info)\n",
    "    \n",
    "    features = bow.predict(kmeans, features);\n",
    "    \n",
    "    labels = keras.utils.to_categorical(labels, num_classes=251)\n",
    "    loss, acc = model.evaluate(features, labels);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
